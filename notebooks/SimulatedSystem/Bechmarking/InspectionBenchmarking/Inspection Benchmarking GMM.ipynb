{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d304004-a4e6-45ee-8cbf-ee2aa5f10935",
   "metadata": {},
   "source": [
    "# Inspection Benchmarking\n",
    "\n",
    "In this study we wish to check the AI's ability to check the validity of the data and determine if the experiment has succeeded or not.\n",
    "\n",
    "We generate 100 figures with synthetic data for each experiment. 100 success and different types of failure.\n",
    "\n",
    "We report the correctness with 1: only fitting 2: only vision model 3: vision model + fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c6810e-6093-443e-930c-5d5e6865d315",
   "metadata": {},
   "outputs": [],
   "source": [
    "import leeq\n",
    "import json\n",
    "from simulated_setup import * # Change to your customized setup file\n",
    "import numpy as np\n",
    "from scipy import optimize as so\n",
    "from leeq.experiments.builtin import *\n",
    "import plotly.graph_objects as go\n",
    "from labchronicle import log_and_record, register_browser_function\n",
    "\n",
    "from leeq.utils.compatibility import *\n",
    "from leeq.core.elements.built_in.qudit_transmon import TransmonElement\n",
    "from leeq.experiments.builtin import *\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d284ea37-b893-4868-ac2d-591ec157dc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dut_dict = {\n",
    "    'Q1': {'Active': True, 'Tuneup': False,'FromLog':False, 'Params': configuration_a},\n",
    "    'Q2': {'Active': True, 'Tuneup': False,'FromLog':False, 'Params': configuration_b}\n",
    "} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43ac3aa-e3b4-406d-a981-c48fa9880b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "duts_dict = {}\n",
    "for hrid, dd in dut_dict.items():\n",
    "    if (dd['Active']):\n",
    "        if (dd['FromLog']):\n",
    "            dut = TransmonElement.load_from_calibration_log(dd['Params']['hrid'])\n",
    "        else:\n",
    "            dut = TransmonElement(name=dd['Params']['hrid'],parameters=dd['Params'])\n",
    "            \n",
    "        dut.print_config_info()\n",
    "        duts_dict[hrid] = dut\n",
    "\n",
    "dut = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31e02e3-b653-485c-9a01-92f6409b99bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af187d4-4824-491f-ba0f-a8d063c038b3",
   "metadata": {},
   "source": [
    "## GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d544a993-19e7-49cd-9722-21f3d54ffd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_benchmark(scan_params):\n",
    "    dut = duts_dict['Q1']  \n",
    "    res = MeasurementCalibrationMultilevelGMM(dut=dut,**scan_params)\n",
    "    return extract_results_from_experiment(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1300a25b-ba84-41e8-949e-e4703dfa1851",
   "metadata": {},
   "outputs": [],
   "source": [
    "success_inspections = {}\n",
    "\n",
    "for i in range(num_samples):\n",
    "    np.random.seed(i)\n",
    "\n",
    "    simulation_setup()\n",
    "    setup().status().set_param(\"AIAutoInspectPlots\", True)  \n",
    "    ExperimentManager().status().set_parameter(\"Plot_Result_In_Jupyter\",True)\n",
    "    \n",
    "    scan_params = {\n",
    "        'amp' : 0.1+0.05*np.random.random(1)[0],\n",
    "        'freq': 9645.5+1*(0.5-np.random.random(1)[0]),\n",
    "    }\n",
    "    print(scan_params)\n",
    "        \n",
    "    result = run_benchmark(scan_params)\n",
    "\n",
    "    success_inspections[i] = result\n",
    "    with open('gmm_success_cases.json', 'w') as f:\n",
    "        json.dump(success_inspections, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80613851-0a2b-44f4-aed2-3284609ac5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_inspections = {}\n",
    "\n",
    "for i in range(num_samples):\n",
    "    np.random.seed(i)\n",
    "\n",
    "    offset = 1000+ 10 * np.random.random(1)[0]+2\n",
    "\n",
    "    simulation_setup()\n",
    "    setup().status().set_param(\"AIAutoInspectPlots\", True)  \n",
    "    ExperimentManager().status().set_parameter(\"Plot_Result_In_Jupyter\",True)\n",
    "    \n",
    "    scan_params = {\n",
    "        'amp' : 0.1+0.05*np.random.random(1)[0],\n",
    "        'freq': 9645.5-10+1*(0.5-np.random.random(1)[0]),\n",
    "    }\n",
    "\n",
    "    result = run_benchmark(scan_params)\n",
    "    \n",
    "    failed_inspections[i] = result\n",
    "    with open('gmm_failed_cases.json', 'w') as f:\n",
    "        json.dump(failed_inspections, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25240a2b-bdbc-4659-9413-80b4a1dbae07",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation_setup()\n",
    "dut = duts_dict['Q1']  \n",
    "meas = MeasurementCalibrationMultilevelGMM(dut=dut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dca8e7c-1ddc-4019-b28e-97078e7b1a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "meas.hexbin_iq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79a5722-092b-4d21-ae50-5e5d2db8c8a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
