{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ai-integration-intro",
   "metadata": {},
   "source": [
    "# 05 - AI-Assisted Experiment Generation\n",
    "\n",
    "This notebook demonstrates LeeQ's unique AI integration capabilities for automated experiment generation and optimization.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand AI-assisted experiment design\n",
    "- Learn about automated parameter optimization\n",
    "- Practice with intelligent calibration workflows\n",
    "- Explore LLM integration for experiment planning\n",
    "\n",
    "## Prerequisites\n",
    "- Complete [04_calibration.ipynb](04_calibration.ipynb)\n",
    "- Understanding of machine learning concepts (helpful but not required)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-section",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "import-modules",
   "metadata": {},
   "outputs": [],
   "source": "# Import required modules for AI integration\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nfrom datetime import datetime\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Core LeeQ imports\nfrom leeq.chronicle import Chronicle, log_and_record\nfrom leeq.core.elements.built_in.qudit_transmon import TransmonElement\nfrom leeq.setups.built_in.setup_simulation_high_level import HighLevelSimulationSetup\nfrom leeq.experiments.experiments import ExperimentManager\nfrom leeq.theory.simulation.numpy.rotated_frame_simulator import VirtualTransmon\n\n# Import experiment modules\nfrom leeq.experiments.builtin.basic.calibrations import (\n    RabiAmplitudeCalibration,\n    MeasurementStatistics\n)\nfrom leeq.experiments.builtin.basic.characterizations import (\n    T1Measurement,\n    T2RamseyMeasurement\n)\n\n# Start Chronicle logging\nChronicle().start_log()\nlog_and_record(\"ai_integration_tutorial_start\", {\n    \"notebook\": \"05_ai_integration\",\n    \"timestamp\": datetime.now().isoformat()\n})\n\nprint(\"AI Integration modules imported successfully!\")\nprint(\"Chronicle logging started for AI tutorial\")\n\n# Create AI-optimized virtual quantum system\nmanager = ExperimentManager()\nmanager.clear_setups()\n\n# Create virtual transmon with AI-optimized parameters\nai_qubit = VirtualTransmon(\n    name=\"AIOptimizedQubit\",\n    qubit_frequency=5042.3,\n    anharmonicity=-201,\n    t1=85,  # Optimized coherence times\n    t2=45,\n    readout_frequency=9647.8,\n    quiescent_state_distribution=np.asarray([0.88, 0.10, 0.015, 0.005])\n)\n\n# Setup high-level simulation\nsetup = HighLevelSimulationSetup(\n    name='AIAssistedSetup',\n    virtual_qubits={1: ai_qubit}\n)\nmanager.register_setup(setup)\n\nprint(f\"AI-optimized quantum system initialized:\")\nprint(f\"  Qubit: {ai_qubit.name}\")\nprint(f\"  Frequency: {ai_qubit.qubit_frequency} MHz\")\nprint(f\"  T1: {ai_qubit.t1} Œºs, T2: {ai_qubit.t2} Œºs\")"
  },
  {
   "cell_type": "markdown",
   "id": "ai-experiment-design",
   "metadata": {},
   "source": "## AI-Assisted Experiment Design\n\nLeeQ's AI integration allows for intelligent experiment design and optimization. AI can:\n\n1. **Generate Experiment Sequences**: Automatically create optimal measurement sequences\n2. **Parameter Optimization**: Use machine learning to find optimal control parameters\n3. **Adaptive Protocols**: Adjust experiments based on real-time results\n4. **Error Mitigation**: Intelligently design error correction sequences\n5. **Resource Optimization**: Minimize experimental time while maximizing information gain\n\n### AI-Powered Experiment Generator\n\nThe AI system analyzes your quantum system and experimental goals to suggest optimal protocols."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ai-experiment-code",
   "metadata": {},
   "outputs": [],
   "source": "class AIExperimentGenerator:\n    \"\"\"\n    AI-assisted experiment generation and optimization.\n    \"\"\"\n    \n    def __init__(self, quantum_system):\n        self.system = quantum_system\n        self.experiment_library = self._initialize_library()\n        self.optimization_history = []\n        \n    def _initialize_library(self):\n        \"\"\"Initialize library of experiment templates.\"\"\"\n        return {\n            'characterization': {\n                'rabi': {'priority': 'high', 'time_cost': 5, 'info_gain': 0.9},\n                't1': {'priority': 'medium', 'time_cost': 10, 'info_gain': 0.7},\n                't2_ramsey': {'priority': 'medium', 'time_cost': 8, 'info_gain': 0.6},\n                't2_echo': {'priority': 'low', 'time_cost': 12, 'info_gain': 0.5}\n            },\n            'calibration': {\n                'frequency': {'priority': 'high', 'time_cost': 3, 'info_gain': 0.95},\n                'amplitude': {'priority': 'high', 'time_cost': 4, 'info_gain': 0.9},\n                'drag': {'priority': 'medium', 'time_cost': 15, 'info_gain': 0.6}\n            }\n        }\n        \n    def generate_optimal_sequence(self, time_budget=30, objectives=['calibration']):\n        \"\"\"\n        Generate optimal experiment sequence given time budget and objectives.\n        \"\"\"\n        print(f\"\\nü§ñ AI Experiment Generator\")\n        print(f\"Time budget: {time_budget} minutes\")\n        print(f\"Objectives: {objectives}\")\n        print(\"=\" * 50)\n        \n        # Collect candidate experiments\n        candidates = []\n        for category in objectives:\n            if category in self.experiment_library:\n                for exp_name, props in self.experiment_library[category].items():\n                    candidates.append({\n                        'name': exp_name,\n                        'category': category,\n                        'time': props['time_cost'],\n                        'gain': props['info_gain'],\n                        'priority': props['priority'],\n                        'efficiency': props['info_gain'] / props['time_cost']\n                    })\n        \n        # AI optimization: maximize information gain within time budget\n        selected_experiments = self._optimize_sequence(candidates, time_budget)\n        \n        return selected_experiments\n    \n    def _optimize_sequence(self, candidates, time_budget):\n        \"\"\"Optimize experiment sequence using greedy algorithm.\"\"\"\n        # Sort by efficiency (info gain per unit time)\n        candidates.sort(key=lambda x: x['efficiency'], reverse=True)\n        \n        selected = []\n        remaining_time = time_budget\n        total_gain = 0\n        \n        for exp in candidates:\n            if exp['time'] <= remaining_time:\n                selected.append(exp)\n                remaining_time -= exp['time']\n                total_gain += exp['gain']\n                \n        print(f\"\\nüéØ Optimized Sequence:\")\n        for i, exp in enumerate(selected, 1):\n            efficiency_score = exp['efficiency'] * 100\n            print(f\"  {i}. {exp['name'].upper()} ({exp['category']})\")\n            print(f\"     Time: {exp['time']}min, Gain: {exp['gain']:.2f}, Efficiency: {efficiency_score:.0f}%\")\n            \n        print(f\"\\nüìä Summary:\")\n        print(f\"  Total time used: {time_budget - remaining_time}/{time_budget} minutes\")\n        print(f\"  Total information gain: {total_gain:.2f}\")\n        print(f\"  Time efficiency: {(time_budget - remaining_time)/time_budget*100:.1f}%\")\n        \n        return selected\n    \n    def adaptive_parameter_optimization(self, experiment_type, initial_params):\n        \"\"\"\n        Use AI to adaptively optimize experimental parameters.\n        \"\"\"\n        print(f\"\\nüß† Adaptive Parameter Optimization for {experiment_type}\")\n        print(\"=\" * 50)\n        \n        # Simulate iterative optimization\n        current_params = initial_params.copy()\n        optimization_trace = []\n        \n        for iteration in range(5):  # 5 optimization rounds\n            # Simulate running experiment with current parameters\n            fidelity = self._simulate_experiment_fidelity(experiment_type, current_params)\n            \n            # Log iteration\n            optimization_trace.append({\n                'iteration': iteration + 1,\n                'params': current_params.copy(),\n                'fidelity': fidelity\n            })\n            \n            print(f\"Iteration {iteration + 1}: Fidelity = {fidelity:.4f}\")\n            \n            # AI-suggested parameter updates\n            if iteration < 4:  # Don't update on last iteration\n                current_params = self._suggest_parameter_updates(current_params, fidelity)\n                \n        # Find best parameters\n        best_iteration = max(optimization_trace, key=lambda x: x['fidelity'])\n        \n        print(f\"\\n‚ú® Optimization Complete!\")\n        print(f\"Best fidelity: {best_iteration['fidelity']:.4f} (iteration {best_iteration['iteration']})\")\n        print(f\"Improvement: {best_iteration['fidelity'] - optimization_trace[0]['fidelity']:+.4f}\")\n        \n        return best_iteration['params'], optimization_trace\n    \n    def _simulate_experiment_fidelity(self, exp_type, params):\n        \"\"\"Simulate experiment fidelity for given parameters.\"\"\"\n        # Base fidelity with some noise\n        base_fidelity = 0.95\n        \n        # Parameter-dependent improvements/degradations\n        if exp_type == 'rabi':\n            # Optimal amplitude around 0.5\n            amplitude_penalty = abs(params.get('amplitude', 0.5) - 0.5) * 0.1\n            fidelity = base_fidelity - amplitude_penalty\n        elif exp_type == 'ramsey':\n            # Optimal frequency close to target\n            freq_penalty = abs(params.get('frequency', 5000) - 5000) / 1000 * 0.05\n            fidelity = base_fidelity - freq_penalty\n        else:\n            fidelity = base_fidelity\n            \n        # Add noise\n        fidelity += np.random.normal(0, 0.005)\n        return max(0, min(1, fidelity))  # Clamp between 0 and 1\n    \n    def _suggest_parameter_updates(self, current_params, current_fidelity):\n        \"\"\"AI-suggested parameter updates based on current performance.\"\"\"\n        new_params = current_params.copy()\n        \n        # Simple gradient-based optimization simulation\n        for param, value in current_params.items():\n            if param == 'amplitude':\n                # Move towards optimal amplitude (0.5)\n                step_size = 0.02\n                if value > 0.5:\n                    new_params[param] = value - step_size\n                else:\n                    new_params[param] = value + step_size\n            elif param == 'frequency':\n                # Random walk with bias towards improvement\n                step_size = np.random.choice([-2, -1, 1, 2])\n                new_params[param] = value + step_size\n                \n        return new_params\n\n# Create and demonstrate AI experiment generator\nai_generator = AIExperimentGenerator(ai_qubit)\n\n# Generate optimal experiment sequence\noptimal_sequence = ai_generator.generate_optimal_sequence(\n    time_budget=25,  # 25 minutes available\n    objectives=['calibration', 'characterization']\n)\n\nlog_and_record(\"ai_experiment_sequence\", {\n    \"sequence\": optimal_sequence,\n    \"time_budget\": 25,\n    \"objectives\": ['calibration', 'characterization']\n})"
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-calibration",
   "metadata": {},
   "source": "## Intelligent Calibration Workflows\n\nAI-optimized calibration procedures use machine learning to:\n\n1. **Predict Parameter Drift**: Anticipate when recalibration is needed\n2. **Adaptive Scheduling**: Adjust calibration frequency based on system stability\n3. **Parameter Correlation**: Learn relationships between different calibration parameters\n4. **Failure Prediction**: Identify early warning signs of calibration failure\n5. **Resource Allocation**: Optimize calibration time vs. accuracy trade-offs\n\n### Intelligent Calibration Manager\n\nThe AI calibration system continuously learns from your quantum system's behavior to optimize calibration protocols."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intelligent-calibration-code",
   "metadata": {},
   "outputs": [],
   "source": "# Demonstrate adaptive parameter optimization with Rabi experiment\nprint(\"=\" * 70)\nprint(\"AI ADAPTIVE PARAMETER OPTIMIZATION DEMO\")\nprint(\"=\" * 70)\n\n# Example: Optimize Rabi experiment parameters\ninitial_rabi_params = {\n    'amplitude': 0.3,  # Starting point away from optimal\n    'frequency': 5000,\n    'pulse_width': 0.05\n}\n\nprint(\"Starting Rabi parameter optimization...\")\noptimal_rabi_params, rabi_trace = ai_generator.adaptive_parameter_optimization(\n    'rabi', initial_rabi_params\n)\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"AI INTELLIGENT CALIBRATION MANAGER\")\nprint(\"=\"*50)\n\nclass IntelligentCalibrationManager:\n    \"\"\"\n    AI-powered calibration optimization and scheduling.\n    \"\"\"\n    \n    def __init__(self, qubit_element):\n        self.qubit = qubit_element\n        self.calibration_history = []\n        self.drift_model = None\n        self.stability_score = 1.0\n        \n    def predict_parameter_drift(self, time_horizon_hours=8):\n        \"\"\"\n        Predict parameter drift using AI model.\n        \"\"\"\n        print(f\"\\nüîÆ Parameter Drift Prediction (next {time_horizon_hours} hours)\")\n        print(\"=\" * 55)\n        \n        # Simulate drift prediction model\n        current_time = datetime.now()\n        \n        # Simulate different parameters with different drift patterns\n        predictions = {\n            'frequency': {\n                'current': 5042.3,\n                'predicted': 5042.3 + 0.5 * np.sin(time_horizon_hours/12 * np.pi),\n                'confidence': 0.85,\n                'drift_rate': 0.06  # MHz/hour\n            },\n            'amplitude': {\n                'current': 0.5487,\n                'predicted': 0.5487 + 0.002 * time_horizon_hours,\n                'confidence': 0.92,\n                'drift_rate': 0.0003  # per hour\n            },\n            't1': {\n                'current': 85,\n                'predicted': 85 - 0.5 * time_horizon_hours,\n                'confidence': 0.73,\n                'drift_rate': -0.5  # Œºs/hour\n            }\n        }\n        \n        for param, data in predictions.items():\n            current = data['current']\n            predicted = data['predicted']\n            drift = predicted - current\n            confidence = data['confidence']\n            \n            status = \"üü¢\" if abs(drift) < 0.1 * current else \"üü°\" if abs(drift) < 0.2 * current else \"üî¥\"\n            \n            print(f\"  {param.upper()}:\")\n            print(f\"    Current: {current:.4f}\")\n            print(f\"    Predicted: {predicted:.4f} (Œî = {drift:+.4f})\")\n            print(f\"    Confidence: {confidence:.0%} {status}\")\n            print()\n            \n        return predictions\n    \n    def adaptive_calibration_schedule(self):\n        \"\"\"\n        Generate adaptive calibration schedule based on system stability.\n        \"\"\"\n        print(\"\\nüìÖ Adaptive Calibration Schedule\")\n        print(\"=\" * 40)\n        \n        # Calculate system stability metrics\n        stability_metrics = {\n            'frequency_stability': 0.85,  # How stable is frequency\n            'amplitude_stability': 0.92,  # How stable is amplitude\n            'coherence_stability': 0.78,  # How stable are T1/T2\n            'measurement_stability': 0.89  # How stable is readout\n        }\n        \n        # Overall stability score\n        overall_stability = np.mean(list(stability_metrics.values()))\n        self.stability_score = overall_stability\n        \n        print(f\"System Stability Score: {overall_stability:.2f}/1.00\")\n        print()\n        \n        # Generate adaptive schedule\n        if overall_stability > 0.9:\n            schedule_type = \"Relaxed\"\n            calibration_interval = 8  # hours\n            quick_check_interval = 2  # hours\n        elif overall_stability > 0.8:\n            schedule_type = \"Standard\"\n            calibration_interval = 6\n            quick_check_interval = 1.5\n        else:\n            schedule_type = \"Intensive\"\n            calibration_interval = 4\n            quick_check_interval = 1\n            \n        schedule = {\n            'type': schedule_type,\n            'full_calibration_interval': calibration_interval,\n            'quick_check_interval': quick_check_interval,\n            'stability_score': overall_stability\n        }\n        \n        print(f\"Recommended Schedule: {schedule_type}\")\n        print(f\"  Full calibration every: {calibration_interval} hours\")\n        print(f\"  Quick checks every: {quick_check_interval} hours\")\n        \n        return schedule\n    \n    def visualize_parameter_correlation(self):\n        \"\"\"\n        Visualize correlations between calibration parameters.\n        \"\"\"\n        print(\"\\nüîó Parameter Correlation Analysis\")\n        print(\"=\" * 40)\n        \n        # Simulate correlation matrix\n        parameters = ['frequency', 'amplitude', 't1', 't2', 'readout_amp']\n        n_params = len(parameters)\n        \n        # Generate realistic correlation matrix\n        correlations = np.array([\n            [1.00,  0.12, -0.05,  0.08,  0.03],  # frequency\n            [0.12,  1.00,  0.07,  0.15, -0.08],  # amplitude\n            [-0.05, 0.07,  1.00,  0.65, -0.12],  # t1\n            [0.08,  0.15,  0.65,  1.00, -0.18],  # t2\n            [0.03, -0.08, -0.12, -0.18,  1.00]   # readout_amp\n        ])\n        \n        # Create correlation heatmap\n        fig = go.Figure(data=go.Heatmap(\n            z=correlations,\n            x=parameters,\n            y=parameters,\n            colorscale='RdBu',\n            zmid=0,\n            colorbar=dict(title=\"Correlation\")\n        ))\n        \n        fig.update_layout(\n            title='Parameter Correlation Matrix',\n            width=600,\n            height=500\n        )\n        \n        fig.show()\n        \n        # Identify strong correlations\n        strong_correlations = []\n        for i in range(n_params):\n            for j in range(i+1, n_params):\n                corr_val = correlations[i, j]\n                if abs(corr_val) > 0.3:\n                    strength = \"Strong\" if abs(corr_val) > 0.6 else \"Moderate\"\n                    direction = \"positive\" if corr_val > 0 else \"negative\"\n                    strong_correlations.append({\n                        'param1': parameters[i],\n                        'param2': parameters[j],\n                        'correlation': corr_val,\n                        'strength': strength,\n                        'direction': direction\n                    })\n        \n        print(\"\\nSignificant Correlations:\")\n        for corr in strong_correlations:\n            print(f\"  {corr['param1']} ‚Üî {corr['param2']}: {corr['correlation']:+.3f} ({corr['strength']} {corr['direction']})\")\n            \n        return correlations\n\n# Create and demonstrate intelligent calibration manager\nprint(\"Creating Intelligent Calibration Manager...\")\n\n# Create a temporary qubit element for the calibration manager\nqubit_config = {\n    'hrid': 'AI_QUBIT',\n    'lpb_collections': {\n        'f01': {\n            'type': 'SimpleDriveCollection',\n            'freq': 5042.3,\n            'channel': 1,\n            'shape': 'blackman_drag',\n            'amp': 0.5487,\n            'phase': 0.,\n            'width': 0.05,\n            'alpha': 500,\n            'trunc': 1.2\n        }\n    },\n    'measurement_primitives': {\n        '0': {\n            'type': 'SimpleDispersiveMeasurement',\n            'freq': 9647.8,\n            'channel': 1,\n            'shape': 'square',\n            'amp': 0.15,\n            'phase': 0.,\n            'width': 1,\n            'trunc': 1.2,\n            'distinguishable_states': [0, 1]\n        }\n    }\n}\n\nai_qubit_element = TransmonElement(name='AI_QUBIT', parameters=qubit_config)\ncal_manager = IntelligentCalibrationManager(ai_qubit_element)\n\n# Run AI-powered calibration analysis\ndrift_predictions = cal_manager.predict_parameter_drift()\nschedule = cal_manager.adaptive_calibration_schedule()\ncorrelations = cal_manager.visualize_parameter_correlation()\n\n# Log intelligent calibration results\nlog_and_record(\"intelligent_calibration_analysis\", {\n    \"drift_predictions\": drift_predictions,\n    \"adaptive_schedule\": schedule,\n    \"stability_score\": cal_manager.stability_score\n})"
  },
  {
   "cell_type": "markdown",
   "id": "llm-integration",
   "metadata": {},
   "source": "## LLM Integration for Experiment Planning\n\nLarge Language Models (LLMs) can significantly enhance quantum experiment workflows by:\n\n1. **Natural Language Experiment Design**: Describe experiments in plain English\n2. **Code Generation**: Automatically generate LeeQ experiment code\n3. **Data Analysis**: Interpret experimental results and suggest next steps\n4. **Documentation**: Generate experiment reports and documentation\n5. **Troubleshooting**: Diagnose issues and suggest solutions\n6. **Literature Integration**: Connect experimental results with published research\n\n### LLM-Assisted Experiment Planning\n\nThe LLM integration allows you to interact with LeeQ using natural language, making quantum experiments more accessible."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "llm-integration-code",
   "metadata": {},
   "outputs": [],
   "source": "class LLMExperimentAssistant:\n    \"\"\"\n    LLM-powered assistant for quantum experiment planning and analysis.\n    \"\"\"\n    \n    def __init__(self):\n        self.conversation_history = []\n        self.experiment_knowledge_base = self._load_knowledge_base()\n        \n    def _load_knowledge_base(self):\n        \"\"\"Load quantum experiment knowledge base.\"\"\"\n        return {\n            'experiment_types': {\n                'rabi': {\n                    'description': 'Measure qubit control by varying drive amplitude',\n                    'outputs': 'pi_pulse_amplitude, rabi_frequency',\n                    'time_estimate': '5-10 minutes',\n                    'prerequisites': 'rough_frequency_calibration'\n                },\n                't1': {\n                    'description': 'Measure energy relaxation time',\n                    'outputs': 'relaxation_time_t1',\n                    'time_estimate': '10-15 minutes',\n                    'prerequisites': 'pi_pulse_calibration'\n                },\n                'ramsey': {\n                    'description': 'Measure dephasing time and fine-tune frequency',\n                    'outputs': 't2_star, frequency_offset',\n                    'time_estimate': '8-12 minutes',\n                    'prerequisites': 'pi_half_pulse_calibration'\n                }\n            },\n            'troubleshooting': {\n                'low_contrast': ['check_measurement_parameters', 'verify_pulse_calibration', 'check_mixer_calibration'],\n                'high_leakage': ['adjust_drag_parameters', 'reduce_pulse_amplitude', 'check_anharmonicity'],\n                'unstable_frequency': ['check_temperature_stability', 'verify_flux_noise', 'recalibrate_more_frequently']\n            }\n        }\n    \n    def natural_language_query(self, query):\n        \"\"\"\n        Process natural language queries about quantum experiments.\n        \"\"\"\n        print(f\"\\nü§ñ LLM Assistant: Processing query...\")\n        print(f\"Query: \\\"{query}\\\"\")\n        print(\"=\" * 60)\n        \n        # Simple keyword-based response system (simulating LLM)\n        query_lower = query.lower()\n        \n        if 'rabi' in query_lower and ('run' in query_lower or 'perform' in query_lower):\n            response = self._generate_rabi_experiment_code()\n        elif 't1' in query_lower and ('measure' in query_lower or 'run' in query_lower):\n            response = self._generate_t1_experiment_code()\n        elif 'calibrate' in query_lower or 'calibration' in query_lower:\n            response = self._generate_calibration_workflow()\n        elif 'troubleshoot' in query_lower or 'problem' in query_lower:\n            response = self._generate_troubleshooting_guide()\n        elif 'analyze' in query_lower or 'results' in query_lower:\n            response = self._generate_analysis_suggestions()\n        else:\n            response = self._generate_general_guidance(query_lower)\n            \n        # Log conversation\n        self.conversation_history.append({\n            'query': query,\n            'response': response,\n            'timestamp': datetime.now().isoformat()\n        })\n        \n        return response\n    \n    def _generate_rabi_experiment_code(self):\n        \"\"\"Generate code for Rabi experiment.\"\"\"\n        code_example = '''# AI-Generated Rabi Experiment Code\nfrom leeq.experiments.builtin.basic.calibrations import RabiAmplitudeCalibration\n\n# Create Rabi experiment with optimized parameters\nrabi_experiment = RabiAmplitudeCalibration(\n    name=\"AI_Rabi_Calibration\",\n    qubit=1,\n    drive_frequency=ai_qubit.qubit_frequency,  # Use current best estimate\n    amplitude_start=0.0,\n    amplitude_stop=1.0,\n    amplitude_points=31,  # Good resolution without excessive time\n    pulse_width=0.05,  # Standard 50ns pulse\n    repeated_measurement_count=1000  # High statistics for accurate calibration\n)\n\n# Run the experiment\nresults = rabi_experiment.run()\n\n# Extract œÄ-pulse amplitude\npi_amplitude = results['sweep_values'][np.argmax(results['measurement_probabilities'])]\nprint(f\"Calibrated œÄ-pulse amplitude: {pi_amplitude:.4f}\")'''\n        \n        return {\n            'response_type': 'code_generation',\n            'explanation': \"I've generated a Rabi experiment optimized for your system. This will sweep drive amplitude to find the œÄ-pulse amplitude needed for reliable qubit control.\",\n            'code': code_example,\n            'estimated_time': '5-8 minutes',\n            'next_steps': ['Run the generated code', 'Analyze Rabi oscillations', 'Update pulse calibration']\n        }\n    \n    def _generate_t1_experiment_code(self):\n        \"\"\"Generate code for T1 measurement.\"\"\"\n        code_example = '''# AI-Generated T1 Measurement Code\nfrom leeq.experiments.builtin.basic.characterizations import T1Measurement\nfrom scipy.optimize import curve_fit\n\n# T1 experiment with AI-optimized parameters\nt1_experiment = T1Measurement(\n    name=\"AI_T1_Characterization\",\n    qubit=1,\n    delay_start=0.0,\n    delay_stop=200.0,  # Generous range for reliable fit\n    delay_points=31,\n    pi_pulse_amplitude=pi_amplitude,  # Use calibrated amplitude\n    repeated_measurement_count=800\n)\n\n# Run measurement\nresults = t1_experiment.run()\n\n# AI-assisted fitting\ndef exponential_decay(t, amplitude, t1, offset):\n    return amplitude * np.exp(-t / t1) + offset\n\n# Fit T1 decay\npopt, _ = curve_fit(exponential_decay, results['sweep_values'], \n                   results['measurement_probabilities'])\n                   \nt1_measured = popt[1]\nprint(f\"Measured T1: {t1_measured:.1f} Œºs\")'''\n        \n        return {\n            'response_type': 'code_generation',\n            'explanation': \"Here's a T1 measurement protocol optimized for your qubit parameters. The delay range is set based on your qubit's expected T1 time.\",\n            'code': code_example,\n            'estimated_time': '10-12 minutes',\n            'next_steps': ['Verify exponential decay fit', 'Compare with specification', 'Track T1 over time']\n        }\n    \n    def _generate_calibration_workflow(self):\n        \"\"\"Generate comprehensive calibration workflow.\"\"\"\n        return {\n            'response_type': 'workflow_guidance',\n            'explanation': \"Here's an AI-optimized calibration workflow for your quantum system:\",\n            'workflow_steps': [\n                {'step': 1, 'task': 'Resonator Spectroscopy', 'time': '3 min', 'priority': 'High'},\n                {'step': 2, 'task': 'Qubit Spectroscopy', 'time': '5 min', 'priority': 'High'},\n                {'step': 3, 'task': 'Rabi Calibration', 'time': '6 min', 'priority': 'High'},\n                {'step': 4, 'task': 'Ramsey Fine-tuning', 'time': '4 min', 'priority': 'Medium'},\n                {'step': 5, 'task': 'DRAG Optimization', 'time': '12 min', 'priority': 'Medium'},\n                {'step': 6, 'task': 'T1/T2 Characterization', 'time': '15 min', 'priority': 'Low'}\n            ],\n            'total_time': '45 minutes',\n            'optimization_notes': [\n                \"Steps 1-3 are critical and should be done first\",\n                \"DRAG optimization can be skipped if leakage is < 1%\",\n                \"T1/T2 can be done less frequently (daily vs hourly)\"\n            ]\n        }\n    \n    def _generate_troubleshooting_guide(self):\n        \"\"\"Generate troubleshooting recommendations.\"\"\"\n        return {\n            'response_type': 'troubleshooting',\n            'common_issues': [\n                {\n                    'issue': 'Low Rabi Contrast',\n                    'symptoms': ['Excited state probability < 0.8', 'Noisy oscillations'],\n                    'solutions': ['Check mixer calibration', 'Verify pulse amplitude', 'Check T1 time'],\n                    'priority': 'High'\n                },\n                {\n                    'issue': 'Frequency Instability',\n                    'symptoms': ['Ramsey fringes drifting', 'Inconsistent spectroscopy'],\n                    'solutions': ['Check temperature stability', 'Increase calibration frequency', 'Check flux noise'],\n                    'priority': 'Medium'\n                },\n                {\n                    'issue': 'High Measurement Error',\n                    'symptoms': ['Poor state discrimination', 'Low readout SNR'],\n                    'solutions': ['Optimize readout amplitude', 'Check resonator frequency', 'Verify integration time'],\n                    'priority': 'High'\n                }\n            ],\n            'diagnostic_steps': [\n                \"1. Run basic measurement statistics\",\n                \"2. Check Rabi oscillations\",\n                \"3. Verify resonator frequency\",\n                \"4. Measure coherence times\"\n            ]\n        }\n    \n    def _generate_analysis_suggestions(self):\n        \"\"\"Generate data analysis suggestions.\"\"\"\n        return {\n            'response_type': 'analysis_guidance',\n            'analysis_checklist': [\n                \"üìä Verify fit quality (R¬≤ > 0.95 for exponential decays)\",\n                \"üìà Check for systematic trends in residuals\",\n                \"üîç Compare results with previous calibrations\",\n                \"‚ö†Ô∏è  Flag any parameters outside expected ranges\",\n                \"üìù Document any anomalies for future reference\"\n            ],\n            'visualization_tips': [\n                \"Use log-scale plots for exponential decays\",\n                \"Plot residuals to check fit quality\",\n                \"Show error bars for statistical uncertainty\",\n                \"Include reference lines for specifications\"\n            ],\n            'reporting_suggestions': [\n                \"Include experimental parameters and conditions\",\n                \"Report uncertainties with appropriate precision\",\n                \"Add trend analysis for time-series data\",\n                \"Note any deviations from expected behavior\"\n            ]\n        }\n    \n    def _generate_general_guidance(self, query):\n        \"\"\"Generate general guidance for unrecognized queries.\"\"\"\n        return {\n            'response_type': 'general_guidance',\n            'message': \"I can help you with quantum experiments! Try asking about:\",\n            'capabilities': [\n                \"üî¨ Running specific experiments (Rabi, T1, T2, etc.)\",\n                \"üõ†Ô∏è  Calibration procedures and workflows\",\n                \"üîß Troubleshooting experimental issues\",\n                \"üìä Data analysis and visualization\",\n                \"üìö Best practices and optimization tips\"\n            ],\n            'example_queries': [\n                \"\\\"Run a Rabi experiment on qubit 1\\\"\",\n                \"\\\"How do I calibrate my system?\\\"\",\n                \"\\\"My Rabi contrast is low, what should I check?\\\"\",\n                \"\\\"Analyze my T1 measurement results\\\"\"\n            ]\n        }\n    \n    def display_response(self, response):\n        \"\"\"Display formatted response.\"\"\"\n        if response['response_type'] == 'code_generation':\n            print(f\"üí° {response['explanation']}\")\n            print(f\"\\n‚è±Ô∏è  Estimated time: {response['estimated_time']}\")\n            print(f\"\\n```python\")\n            print(response['code'])\n            print(f\"```\")\n            print(f\"\\nüìã Next steps:\")\n            for step in response['next_steps']:\n                print(f\"  ‚Ä¢ {step}\")\n                \n        elif response['response_type'] == 'workflow_guidance':\n            print(f\"üí° {response['explanation']}\")\n            print(f\"\\nüìã Workflow Steps (Total: {response['total_time']}):\")\n            for step in response['workflow_steps']:\n                priority_emoji = \"üî¥\" if step['priority'] == 'High' else \"üü°\" if step['priority'] == 'Medium' else \"üü¢\"\n                print(f\"  {step['step']}. {step['task']} ({step['time']}) {priority_emoji}\")\n            print(f\"\\nüí° Optimization Notes:\")\n            for note in response['optimization_notes']:\n                print(f\"  ‚Ä¢ {note}\")\n                \n        elif response['response_type'] == 'troubleshooting':\n            print(f\"üîß Common Issues and Solutions:\")\n            for issue in response['common_issues']:\n                priority_emoji = \"üî¥\" if issue['priority'] == 'High' else \"üü°\"\n                print(f\"\\n{priority_emoji} {issue['issue']}:\")\n                print(f\"  Symptoms: {', '.join(issue['symptoms'])}\")\n                print(f\"  Solutions: {', '.join(issue['solutions'])}\")\n            print(f\"\\nüîç Diagnostic Steps:\")\n            for step in response['diagnostic_steps']:\n                print(f\"  {step}\")\n                \n        elif response['response_type'] == 'analysis_guidance':\n            print(f\"üìä Analysis Checklist:\")\n            for item in response['analysis_checklist']:\n                print(f\"  {item}\")\n            print(f\"\\nüìà Visualization Tips:\")\n            for tip in response['visualization_tips']:\n                print(f\"  ‚Ä¢ {tip}\")\n            print(f\"\\nüìù Reporting Suggestions:\")\n            for suggestion in response['reporting_suggestions']:\n                print(f\"  ‚Ä¢ {suggestion}\")\n                \n        else:  # general_guidance\n            print(f\"üí° {response['message']}\")\n            print(f\"\\nüîß My Capabilities:\")\n            for capability in response['capabilities']:\n                print(f\"  {capability}\")\n            print(f\"\\nüí¨ Example Queries:\")\n            for example in response['example_queries']:\n                print(f\"  {example}\")\n\n# Create and demonstrate LLM assistant\nllm_assistant = LLMExperimentAssistant()\n\nprint(\"\\n\" + \"#\"*70)\nprint(\"#\" + \" \"*23 + \"LLM EXPERIMENT ASSISTANT\" + \" \"*22 + \"#\")\nprint(\"#\"*70)\n\n# Example interactions\nqueries_and_responses = [\n    \"I need to run a Rabi experiment to calibrate my pi pulse amplitude\",\n    \"What's the best calibration workflow for my qubit?\",\n    \"My Rabi contrast is really low, help me troubleshoot this problem\"\n]\n\nfor i, query in enumerate(queries_and_responses, 1):\n    print(f\"\\n{'='*70}\")\n    print(f\"EXAMPLE {i}: Natural Language Query\")\n    print('='*70)\n    \n    response = llm_assistant.natural_language_query(query)\n    llm_assistant.display_response(response)\n    \n# Log LLM interactions\nlog_and_record(\"llm_assistant_interactions\", {\n    \"interactions\": llm_assistant.conversation_history,\n    \"query_types\": [\"rabi_calibration\", \"workflow_guidance\", \"troubleshooting\"]\n})\n\nprint(\"\\n\" + \"#\"*70)\nprint(\"# AI INTEGRATION TUTORIAL COMPLETE - All systems demonstrated! #\")\nprint(\"#\"*70)"
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations! You've completed the LeeQ tutorial series. You now have a solid foundation in:\n",
    "\n",
    "- LeeQ core concepts and simulation\n",
    "- Single and multi-qubit experiments\n",
    "- Complete calibration workflows\n",
    "- AI-assisted experiment generation\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Explore the following resources for deeper learning:\n",
    "\n",
    "- **Examples**: Check out `/notebooks/examples/` for specific experiment implementations\n",
    "- **Workflows**: See `/notebooks/workflows/` for complete experimental procedures\n",
    "- **Documentation**: Visit the full documentation for detailed API references"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}